# ðŸš€ Distributed LLM Inference (Layer-wise Model Parallelism)

This project demonstrates **distributed inference of a Large Language Model (LLM)** by splitting model layers across **multiple machines (Google Colab instances)** and coordinating them using an **orchestrator server**.

It is designed to simulate **real-world distributed systems**, where limited hardware collaboratively serves LLM inference.

---

## ðŸ§  Architecture Overview

![Distributed LLM Architecture](distributed_llm_architecture.png)

---

